{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Markov Chain (MCMC) Simulation with Metropolis-Hastings Algorithm\n",
    "\n",
    "This tutorial demonstrates how to implement a Monte Carlo Markov Chain using the Metropolis-Hastings algorithm for parameter estimation in cosmology. We will use the Cosmic Microwave Background (CMB) power spectra data simulated using the `camb` library.\n",
    "\n",
    "## Key Concepts:\n",
    "- **CAMB**: A software package used to simulate the CMB power spectra.\n",
    "- **Metropolis-Hastings Algorithm**: A method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult.\n",
    "- **Likelihood Computation**: Using the CMB power spectra to compute likelihoods.\n",
    "- **Priors and Proposals**: Setting up the prior distributions and proposal distributions for our MCMC sampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the camb package if not already installed\n",
    "# !pip install camb\n",
    "\n",
    "# Import necessary libraries\n",
    "import camb\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from camb.model import CAMBParamRangeError\n",
    "from scipy.stats import norm\n",
    "from scipy.io import FortranFile\n",
    "import scipy.linalg\n",
    "\n",
    "# from numba import jit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Power Spectrum Calculation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_spectra(params, lensed=True, accuracy_boost=1, l_sample_boost=1, l_accuracy_boost=1):\n",
    "    \"\"\"\n",
    "    Calculates the CMB power spectra given cosmological parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - params (dict): Cosmological parameters\n",
    "    - lensed (bool): Whether to use lensed spectra\n",
    "    - accuracy_boost (float): Increase accuracy of calculations\n",
    "    - l_sample_boost (float): Increase l sample boost\n",
    "    - l_accuracy_boost (float): Increase l accuracy boost\n",
    "    \"\"\"\n",
    "    # Additional settings for lower accuracy to speed up simulations\n",
    "    params.update({\n",
    "        'AccuracyBoost': accuracy_boost,\n",
    "        'lSampleBoost': l_sample_boost,\n",
    "        'lAccuracyBoost': l_accuracy_boost,\n",
    "        'DoLateRadTruncation': True\n",
    "    })\n",
    "\n",
    "    # Set and get results from CAMB\n",
    "    pars = camb.set_params(**params)\n",
    "    results = camb.get_results(pars)\n",
    "    powers = results.get_cmb_power_spectra(pars, CMB_unit='muK')\n",
    "\n",
    "    # Select either lensed or unlensed spectra\n",
    "    cl = powers['total'] if lensed else powers['unlensed_scalar']\n",
    "\n",
    "    # Remove monopole and dipole terms and return TT, TE, and EE spectra\n",
    "    cl_cut = cl[2:]\n",
    "    return cl_cut[:, 0], cl_cut[:, 3], cl_cut[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Parameter Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_params_and_calculate_spectra(array):\n",
    "    \"\"\"\n",
    "    Adjusts raw parameter array values and calculates power spectra.\n",
    "\n",
    "    Parameters:\n",
    "    - array (numpy.ndarray): Array of parameter values\n",
    "    \"\"\"\n",
    "    array = array.copy()\n",
    "    array[2] /= 100\n",
    "    array[5] = np.exp(array[5]) / 1e10\n",
    "    params = {\n",
    "        'ombh2': array[0],\n",
    "        'omch2': array[1],\n",
    "        'cosmomc_theta': array[2],\n",
    "        'tau': array[3],\n",
    "        'ns': array[4],\n",
    "        'As': array[5],\n",
    "        'mnu': 0.06,\n",
    "        'omk': 0,\n",
    "        'halofit_version': 'mead',\n",
    "        'lmax': 2800\n",
    "    }\n",
    "    return calculate_power_spectra(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Likelihood Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PlanckLitePy:\n",
    "    def __init__(self, data_directory='./MCMC/data', year=2018, spectra='TT', use_low_ell_bins=False):\n",
    "        '''\n",
    "        data_directory = path from where you are running this to the folder\n",
    "          containing the planck2015/8_low_ell and planck2015/8_plik_lite data\n",
    "        year = 2015 or 2018\n",
    "        spectra = TT for just temperature or TTTEEE for temperature (TT),\n",
    "          E mode (EE) and cross (TE) spectra\n",
    "        use_low_ell_bins = True to use 2 low ell bins for the TT 2<=ell<30 data\n",
    "          or False to only use ell>=30\n",
    "        '''\n",
    "        self.year=year\n",
    "        self.spectra=spectra\n",
    "        self.use_low_ell_bins=use_low_ell_bins #False matches Plik_lite - just l>=30\n",
    "\n",
    "        if self.use_low_ell_bins:\n",
    "            self.nbintt_low_ell=2\n",
    "            self.plmin_TT=2\n",
    "        else:\n",
    "            self.nbintt_low_ell=0\n",
    "            self.plmin_TT=30\n",
    "        self.plmin=30\n",
    "        self.plmax=2508\n",
    "        self.calPlanck=1\n",
    "\n",
    "        if year==2015:\n",
    "            self.data_dir=data_directory+'/planck2015_plik_lite/'\n",
    "            version=18\n",
    "        elif year==2018:\n",
    "            self.data_dir=data_directory+'/planck2018_plik_lite/'\n",
    "            version=22\n",
    "        else:\n",
    "            print('Year must be 2015 or 2018')\n",
    "            return 1\n",
    "\n",
    "        if spectra=='TT':\n",
    "            self.use_tt=True\n",
    "            self.use_ee=False\n",
    "            self.use_te=False\n",
    "        elif spectra=='TTTEEE':\n",
    "            self.use_tt=True\n",
    "            self.use_ee=True\n",
    "            self.use_te=True\n",
    "        else:\n",
    "            print('Spectra must be TT or TTTEEE')\n",
    "            return 1\n",
    "\n",
    "        self.nbintt_hi = 215 #30-2508   #used when getting covariance matrix\n",
    "        self.nbinte = 199 #30-1996\n",
    "        self.nbinee = 199 #30-1996\n",
    "        self.nbin_hi=self.nbintt_hi+self.nbinte+self.nbinee\n",
    "\n",
    "        self.nbintt=self.nbintt_hi+self.nbintt_low_ell #mostly want this if using low ell\n",
    "        self.nbin_tot=self.nbintt+self.nbinte+self.nbinee\n",
    "\n",
    "        self.like_file = self.data_dir+'cl_cmb_plik_v'+str(version)+'.dat'\n",
    "        self.cov_file  = self.data_dir+'c_matrix_plik_v'+str(version)+'.dat'\n",
    "        self.blmin_file = self.data_dir+'blmin.dat'\n",
    "        self.blmax_file = self.data_dir+'blmax.dat'\n",
    "        self.binw_file = self.data_dir+'bweight.dat'\n",
    "\n",
    "        # read in binned ell value, C(l) TT, TE and EE and errors\n",
    "        # use_tt etc to select relevant parts\n",
    "        self.bval, self.X_data, self.X_sig=np.genfromtxt(self.like_file, unpack=True)\n",
    "        self.blmin=np.loadtxt(self.blmin_file).astype(int)\n",
    "        self.blmax=np.loadtxt(self.blmax_file).astype(int)\n",
    "        self.bin_w=np.loadtxt(self.binw_file)\n",
    "\n",
    "        if self.use_low_ell_bins:\n",
    "            self.data_dir_low_ell=data_directory+'/planck'+str(year)+'_low_ell/'\n",
    "            self.bval_low_ell, self.X_data_low_ell, self.X_sig_low_ell=np.genfromtxt(self.data_dir_low_ell+'CTT_bin_low_ell_'+str(year)+'.dat', unpack=True)\n",
    "            self.blmin_low_ell=np.loadtxt(self.data_dir_low_ell+'blmin_low_ell.dat').astype(int)\n",
    "            self.blmax_low_ell=np.loadtxt(self.data_dir_low_ell+'blmax_low_ell.dat').astype(int)\n",
    "            self.bin_w_low_ell=np.loadtxt(self.data_dir_low_ell+'bweight_low_ell.dat')\n",
    "\n",
    "            self.bval=np.concatenate((self.bval_low_ell, self.bval))\n",
    "            self.X_data=np.concatenate((self.X_data_low_ell, self.X_data))\n",
    "            self.X_sig=np.concatenate((self.X_sig_low_ell, self.X_sig))\n",
    "\n",
    "            self.blmin_TT=np.concatenate((self.blmin_low_ell, self.blmin+len(self.bin_w_low_ell)))\n",
    "            self.blmax_TT=np.concatenate((self.blmax_low_ell, self.blmax+len(self.bin_w_low_ell)))\n",
    "            self.bin_w_TT=np.concatenate((self.bin_w_low_ell, self.bin_w))\n",
    "\n",
    "        else:\n",
    "            self.blmin_TT=self.blmin\n",
    "            self.blmax_TT=self.blmax\n",
    "            self.bin_w_TT=self.bin_w\n",
    "\n",
    "\n",
    "        self.fisher=self.get_inverse_covmat()\n",
    "\n",
    "    def get_inverse_covmat(self):\n",
    "        # Read full covmat\n",
    "        f = FortranFile(self.cov_file, 'r')\n",
    "        covmat = f.read_reals(dtype=float).reshape((self.nbin_hi, self.nbin_hi))\n",
    "        covmat = np.maximum(covmat, covmat.T)  # Make the matrix symmetric\n",
    "\n",
    "        # Define a dictionary to map conditions to calculations\n",
    "        conditions = {\n",
    "            (True, False, False): (self.nbintt_hi, 0),\n",
    "            (False, False, True): (self.nbinte, self.nbintt_hi),\n",
    "            (False, True, False): (self.nbinee, self.nbintt_hi + self.nbinte),\n",
    "            (True, True, True): (self.nbin_hi, 0)\n",
    "        }\n",
    "\n",
    "        # Select relevant covmat\n",
    "        bin_no, start = conditions.get((self.use_tt, self.use_ee, self.use_te), (None, None))\n",
    "        if bin_no is None:\n",
    "            print(\"not implemented\")\n",
    "            return\n",
    "\n",
    "        end = start + bin_no\n",
    "        cov = covmat[start:end, start:end]\n",
    "\n",
    "        # Invert high ell covariance matrix (cholesky decomposition should be faster)\n",
    "        fisher = scipy.linalg.cho_solve(scipy.linalg.cho_factor(cov), np.identity(bin_no)).T\n",
    "\n",
    "        if self.use_low_ell_bins:\n",
    "            bin_no += self.nbintt_low_ell\n",
    "            inv_covmat_with_lo = np.zeros(shape=(bin_no, bin_no))\n",
    "            inv_covmat_with_lo[0:2, 0:2] = np.diag(1. / self.X_sig_low_ell**2)\n",
    "            inv_covmat_with_lo[2:, 2:] = fisher\n",
    "            fisher = inv_covmat_with_lo\n",
    "\n",
    "        return fisher\n",
    "\n",
    "\n",
    "\n",
    "    # def loglike(self, Cltt, Clte, Clee, ellmin=2):\n",
    "        \n",
    "\n",
    "    #     # If the input is Dl's, convert to Cl's\n",
    "    #     #convert model Dl's to Cls then bin them\n",
    "    #     # ls=np.arange(len(Dltt))+ellmin\n",
    "    #     # fac=ls*(ls+1)/(2*np.pi)\n",
    "    #     # Cltt=Dltt/fac\n",
    "    #     # Clte=Dlte/fac\n",
    "    #     # Clee=Dlee/fac\n",
    "\n",
    "\n",
    "\n",
    "    def binning(self, Cl, blmin, blmax, bin_w, nbins, ellmin):\n",
    "        Cl_bin = np.zeros(nbins)\n",
    "        for i in range(nbins):\n",
    "            Cl_bin[i] = np.sum(Cl[blmin[i]+self.plmin-ellmin:blmax[i]+self.plmin+1-ellmin]*bin_w[blmin[i]:blmax[i]+1])\n",
    "        return Cl_bin\n",
    "\n",
    "    def select_diff_vec(self, Y):\n",
    "        conditions = {\n",
    "            (True, False, False): (self.nbintt, 0),\n",
    "            (False, False, True): (self.nbinte, self.nbintt),\n",
    "            (False, True, False): (self.nbinee, self.nbintt + self.nbinte),\n",
    "            (True, True, True): (self.nbin_tot, 0)\n",
    "        }\n",
    "        bin_no, start = conditions.get((self.use_tt, self.use_ee, self.use_te), (None, None))\n",
    "        if bin_no is None:\n",
    "            print(\"not implemented\")\n",
    "            return None\n",
    "        end = start + bin_no\n",
    "        return Y[start:end]\n",
    "\n",
    "    def loglike(self, Dltt, Dlte, Dlee, ellmin=2):\n",
    "        # Convert model Dl's to Cls then bin them\n",
    "        ls = np.arange(len(Dltt)) + ellmin\n",
    "        fac = ls * (ls + 1) / (2 * np.pi)\n",
    "        Cltt, Clte, Clee = Dltt / fac, Dlte / fac, Dlee / fac\n",
    "\n",
    "        # Bin the Cls\n",
    "        Cltt_bin = self.binning(Cltt, self.blmin_TT, self.blmax_TT, self.bin_w_TT, self.nbintt, ellmin)\n",
    "        Clte_bin = self.binning(Clte, self.blmin, self.blmax, self.bin_w, self.nbinte, ellmin)\n",
    "        Clee_bin = self.binning(Clee, self.blmin, self.blmax, self.bin_w, self.nbinee, ellmin)\n",
    "\n",
    "        # Construct the model\n",
    "        X_model = np.zeros(self.nbin_tot)\n",
    "        X_model[:self.nbintt] = Cltt_bin / self.calPlanck**2\n",
    "        X_model[self.nbintt:self.nbintt+self.nbinte] = Clte_bin / self.calPlanck**2\n",
    "        X_model[self.nbintt+self.nbinte:] = Clee_bin / self.calPlanck**2\n",
    "\n",
    "        # Calculate the difference vector\n",
    "        Y = self.X_data - X_model\n",
    "        diff_vec = self.select_diff_vec(Y)\n",
    "        if diff_vec is None:\n",
    "            return None\n",
    "\n",
    "        return -0.5 * diff_vec.dot(self.fisher.dot(diff_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Prior and Proposal Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior distributions. This is what one would expect to see after the burn-in steps. \n",
    "priors = [\n",
    "    norm(loc=0.0224, scale=np.sqrt(1)*0.00015),\n",
    "    norm(loc=0.12, scale=np.sqrt(1)*0.0015),\n",
    "    norm(loc=1.04109, scale=np.sqrt(1)*0.0004),\n",
    "    norm(loc=0.055, scale=np.sqrt(1)*0.009),\n",
    "    norm(loc=0.965, scale=np.sqrt(1)*0.007),\n",
    "    norm(loc=3.05, scale=np.sqrt(1)*0.01)\n",
    "]\n",
    "\n",
    "# Define the proposal distribution function. This is an example value that works well after the burn-in step. \n",
    "a = 10\n",
    "def proposal_distribution(current_state):\n",
    "    step_sizes = np.array([\n",
    "        0.0001/a, 0.001/a, 0.0002/a, 0.003/15, 0.002/a, 0.01/a\n",
    "    ])\n",
    "    proposal_step = np.random.normal(0, step_sizes)\n",
    "    return current_state + proposal_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the MCMC Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_MH:\n",
    "    def __init__(self, likelihood, proposal_distribution, priors, num_samples, num_chains, stepsize=0.5, burnin_ratio=0.05, resume=False):\n",
    "        self.likelihood = likelihood\n",
    "        self.proposal_distribution = proposal_distribution\n",
    "        self.priors = priors\n",
    "        self.num_samples = num_samples\n",
    "        self.num_chains = num_chains\n",
    "        self.stepsize = stepsize\n",
    "        self.burnin = int(burnin_ratio * num_samples)\n",
    "        self.checkpoint_interval = 250\n",
    "        self.resume = resume\n",
    "        self.R_minus_one = None\n",
    "\n",
    "        self.count = 0\n",
    "\n",
    "        self.curr_state = self.generate_initial_states()\n",
    "        self.curr_likeli = [likelihood.loglike(*process_params_and_calculate_spectra(state)) for state in self.curr_state]\n",
    "        self.samples = [[] for _ in range(num_chains)]\n",
    "        self.likelihoods = [[] for _ in range(num_chains)]\n",
    "        self.accepteds = [0 for _ in range(num_chains)]\n",
    "        self.Rminusones = []\n",
    "\n",
    "\n",
    "        if resume:\n",
    "            self.load_checkpoint()\n",
    "        elif os.path.exists('./checkpoints/checkpoint_burnin.pkl') or os.path.exists('./checkpoints/checkpoint_production.pkl'):\n",
    "            raise ValueError(\"The checkpoint files 'checkpoint_burnin.pkl' and 'checkpoint_production.pkl' should not exist when starting a new run.\")\n",
    "\n",
    "    def generate_initial_states(self):\n",
    "        initial_states = []\n",
    "        for _ in range(self.num_chains):\n",
    "            state = [prior.rvs() for prior in self.priors]\n",
    "            initial_states.append(state)\n",
    "        return initial_states\n",
    "\n",
    "    def save_checkpoint(self, iteration, burn_in=False):\n",
    "        if not os.path.exists('./checkpoints'):\n",
    "            os.makedirs('./checkpoints')\n",
    "        filename = f'./checkpoints/checkpoint_{\"burnin\" if burn_in else \"production\"}.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump((self.curr_state, self.samples, self.likelihoods, self.Rminusones, self.count), f)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoint_file_prod = './checkpoints/checkpoint_production.pkl'\n",
    "        checkpoint_file_burnin = './checkpoints/checkpoint_burnin.pkl'\n",
    "        \n",
    "        if os.path.exists(checkpoint_file_prod):\n",
    "            with open(checkpoint_file_prod, 'rb') as f:\n",
    "                self.curr_state, self.samples, self.likelihoods, self.Rminusones, self.count = pickle.load(f)\n",
    "        elif os.path.exists(checkpoint_file_burnin):\n",
    "            with open(checkpoint_file_burnin, 'rb') as f:\n",
    "                self.curr_state, self.samples, self.likelihoods, self.Rminusones, self.count = pickle.load(f)\n",
    "        else:\n",
    "            raise ValueError(\"No checkpoint files found to resume from.\")\n",
    "\n",
    "    def burn_in(self):\n",
    "        start_time = time.time()\n",
    "        if self.resume:\n",
    "            self.load_checkpoint()\n",
    "\n",
    "        for i in range(self.burnin):\n",
    "            for j in range(self.num_chains):\n",
    "                self.mcmc_updater(j)\n",
    "            if (i + 1) % self.checkpoint_interval == 0:\n",
    "                self.save_checkpoint(i + 1, burn_in=True)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Completed {i+1} burn-in steps in {elapsed_time} seconds\")\n",
    "            self.count += 1\n",
    "\n",
    "    def mcmc_updater(self, chain_index):\n",
    "        proposal_state = self.proposal_distribution(self.curr_state[chain_index])\n",
    "        proposal_state = [max(0, value) for value in proposal_state]\n",
    "\n",
    "        try:\n",
    "            prop_loglikeli = self.likelihood.loglike(*process_params_and_calculate_spectra(proposal_state))\n",
    "        except CAMBParamRangeError:\n",
    "            prop_loglikeli = -np.inf\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            prop_loglikeli = -np.inf\n",
    "\n",
    "        accept_crit = prop_loglikeli - self.curr_likeli[chain_index]\n",
    "        accept_threshold = np.log(np.random.uniform(0, 1))\n",
    "\n",
    "        if accept_crit > accept_threshold:\n",
    "            self.curr_state[chain_index], self.curr_likeli[chain_index] = proposal_state, prop_loglikeli\n",
    "\n",
    "\n",
    "        return self.curr_state[chain_index], self.curr_likeli[chain_index]\n",
    "\n",
    "    def metropolis_hastings(self):\n",
    "        start_time = time.time()\n",
    "        self.burn_in()\n",
    "\n",
    "        if self.resume:\n",
    "            self.load_checkpoint()\n",
    "\n",
    "        print(self.count, self.num_samples)\n",
    "        for i in range(self.count, self.num_samples):\n",
    "            for j in range(self.num_chains):\n",
    "                self.curr_state[j], self.curr_likeli[j] = self.mcmc_updater(j)\n",
    "                self.samples[j].append(self.curr_state[j])\n",
    "                self.likelihoods[j].append(self.curr_likeli[j])\n",
    "\n",
    "            if (i + 1) % self.checkpoint_interval == 0:\n",
    "                self.save_checkpoint(i + 1)\n",
    "                print(f\"Completed {i+1} steps in {time.time() - start_time} seconds\")\n",
    "            if (i + 1) % 50 == 0:\n",
    "                self.calculate_convergence()\n",
    "            \n",
    "            self.count += 1\n",
    "\n",
    "        return self.samples\n",
    "\n",
    "    def calculate_convergence(self):\n",
    "        chain_means = [np.mean(np.array(chain), axis=0) for chain in self.samples]\n",
    "        chain_vars = [np.var(np.array(chain), axis=0, ddof=1) for chain in self.samples]\n",
    "\n",
    "        grand_mean = np.mean(chain_means, axis=0)\n",
    "\n",
    "        B = np.sum((chain_means - grand_mean)**2, axis=0) * len(self.samples[0]) / (len(self.samples) - 1)\n",
    "        W = np.mean(chain_vars, axis=0)\n",
    "\n",
    "        var_plus = (len(self.samples[0]) - 1) / len(self.samples[0]) * W + B / len(self.samples[0])\n",
    "        self.R_minus_one = (var_plus / W - 1).mean()\n",
    "        print(f'R-1 statistic: {self.R_minus_one}')      \n",
    "        self.Rminusones.append(self.R_minus_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m likelihood_function \u001b[38;5;241m=\u001b[39m PlanckLitePy()\n\u001b[1;32m      3\u001b[0m mcmc \u001b[38;5;241m=\u001b[39m MCMC_MH(likelihood_function, proposal_distribution, priors, \u001b[38;5;241m10\u001b[39m, num_chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, stepsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, burnin_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00\u001b[39m, resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmcmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetropolis_hastings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 101\u001b[0m, in \u001b[0;36mMCMC_MH.metropolis_hastings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_chains):\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_state[j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_likeli[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcmc_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[j]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_state[j])\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods[j]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_likeli[j])\n",
      "Cell \u001b[0;32mIn[25], line 75\u001b[0m, in \u001b[0;36mMCMC_MH.mcmc_updater\u001b[0;34m(self, chain_index)\u001b[0m\n\u001b[1;32m     72\u001b[0m proposal_state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m proposal_state]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     prop_loglikeli \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39mloglike(\u001b[38;5;241m*\u001b[39m\u001b[43mprocess_params_and_calculate_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposal_state\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CAMBParamRangeError:\n\u001b[1;32m     77\u001b[0m     prop_loglikeli \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mprocess_params_and_calculate_spectra\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m     10\u001b[0m array[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(array[\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e10\u001b[39m\n\u001b[1;32m     11\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mombh2\u001b[39m\u001b[38;5;124m'\u001b[39m: array[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124momch2\u001b[39m\u001b[38;5;124m'\u001b[39m: array[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlmax\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2800\u001b[39m\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcalculate_power_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mcalculate_power_spectra\u001b[0;34m(params, lensed, accuracy_boost, l_sample_boost, l_accuracy_boost)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Set and get results from CAMB\u001b[39;00m\n\u001b[1;32m     21\u001b[0m pars \u001b[38;5;241m=\u001b[39m camb\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcamb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m powers \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mget_cmb_power_spectra(pars, CMB_unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmuK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Select either lensed or unlensed spectra\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cosmology/lib/python3.9/site-packages/camb/camb.py:37\u001b[0m, in \u001b[0;36mget_results\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _debug_params:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(params)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_power_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/cosmology/lib/python3.9/site-packages/camb/results.py:336\u001b[0m, in \u001b[0;36mCAMBdata.calc_power_spectra\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mCalculates transfer functions and power spectra.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m:param params: optional :class:`~.model.CAMBparams` instance with parameters to use\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_transfers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_transfers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_powers()\n",
      "File \u001b[0;32m~/miniconda3/envs/cosmology/lib/python3.9/site-packages/camb/results.py:316\u001b[0m, in \u001b[0;36mCAMBdata.calc_transfers\u001b[0;34m(self, params, only_transfers, only_time_sources)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (only_transfers \u001b[38;5;129;01mor\u001b[39;00m only_time_sources):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_powers(params)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mCAMBdata_gettransfers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43monly_transfers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43monly_time_sources\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    318\u001b[0m     config\u001b[38;5;241m.\u001b[39mcheck_global_error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalc_transfer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "likelihood_function = PlanckLitePy()\n",
    "\n",
    "mcmc = MCMC_MH(likelihood_function, proposal_distribution, priors, 10, num_chains=4, stepsize=0.1, burnin_ratio=0.00, resume=False)\n",
    "mcmc.metropolis_hastings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
